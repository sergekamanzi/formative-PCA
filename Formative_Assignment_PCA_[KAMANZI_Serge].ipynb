{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergekamanzi/formative-PCA/blob/main/Formative_Assignment_PCA_%5BKAMANZI_Serge%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:300/1*mgncZaKaVx9U6OCQu_m8Bg.jpeg\">\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "The goal of PCA is to extract information while reducing the number of features\n",
        "from a dataset by identifying which existing features relate to another. The crux of the algorithm is trying to determine the relationship between existing features, called principal components, and then quantifying how relevant these principal components are. The principal components are used to transform the high dimensional data to a lower dimensional data while preserving as much information. For a principal component to be relevant, it needs to capture information about the features. We can determine the relationships between features using covariance."
      ],
      "metadata": {
        "id": "xyATLU4z1cYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary package\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "UTntK0eUNimH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/fuel_econ - fuel_econ (1).csv')\n",
        "numeric_data = data.select_dtypes(include=[np.number])"
      ],
      "metadata": {
        "id": "qWaiAdz8PyKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Standardize the Data along the Features\n",
        "\n",
        "![image.png](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLxe5VYCBsaZddkkTZlCY24Yov4JJD4-ArTA&usqp=CAU)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Explain why we need to handle the data on the same scale.\n",
        "\n",
        "answer=  We need to standardize the data along the features, or bring them to the same scale, to ensure that all features contribute equally to the model's learning process"
      ],
      "metadata": {
        "id": "U2U2_Q5ebos3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardizing the data\n",
        "scaler = StandardScaler()\n",
        "standardized_data = scaler.fit_transform(numeric_data)\n",
        "\n",
        "# The standardized data\n",
        "print(standardized_data)"
      ],
      "metadata": {
        "id": "olIR_fEUQawM",
        "outputId": "987c1190-3607-4aee-be98-f52814373578",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.73714048 -1.47583548  0.28310163 ...  1.02283829 -0.95057953\n",
            "  -0.94575548]\n",
            " [-1.73668367 -1.47583548 -0.78181585 ... -0.29854998  0.1886082\n",
            "   0.1942578 ]\n",
            " [-1.73622685 -1.47583548  0.28310163 ...  0.56793413 -0.38098566\n",
            "  -0.37574884]\n",
            " ...\n",
            " [ 1.77804883  1.4747841  -0.78181585 ... -1.78240402  1.89738979\n",
            "   1.90427772]\n",
            " [ 1.77850564  1.4747841   0.28310163 ...  0.11302997 -0.38098566\n",
            "  -0.37574884]\n",
            " [ 1.77896246  1.4747841   0.28310163 ...  0.43796152 -0.95057953\n",
            "  -0.94575548]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![cov matrix.webp](https://dmitry.ai/uploads/default/original/1X/9bd2851674ebb55e404cc3ff5e2ffe65b42ff460.png)\n",
        "\n",
        "We use the pair - wise covariance of the different features to determine how they relate to each other. With these covariances, our goal is to group / cluster based on similar patterns. Intuitively, we can relate features if they have similar covariances with other features."
      ],
      "metadata": {
        "id": "7rzoiQ7fMk_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Calculate the Covariance Matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "uuhux3UEcBgw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn8oujZlK9YR",
        "outputId": "b1ea862c-5b76-4c64-b2b2-0e4967cac424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Covariance Matrix:\n",
            "[[ 1.00025458  0.98591866 -0.06011148 -0.07468488 -0.00657025 -0.02195656\n",
            "   0.09182316  0.09124849  0.0906161   0.09538375  0.09382686 -0.09974229\n",
            "  -0.1279056  -0.12235207]\n",
            " [ 0.98591866  1.00025458 -0.05532701 -0.07044161  0.00623397 -0.03365174\n",
            "   0.06806739  0.06675938  0.07330836  0.07766039  0.07201181 -0.0811853\n",
            "  -0.1498676  -0.14517775]\n",
            " [-0.06011148 -0.05532701  1.00025458  0.93411019  0.24763384 -0.00426546\n",
            "  -0.69327904 -0.66619842 -0.76646982 -0.77169964 -0.73821112  0.84848979\n",
            "  -0.78405759 -0.78201448]\n",
            " [-0.07468488 -0.07044161  0.93411019  1.00025458  0.2594021   0.02207729\n",
            "  -0.71366074 -0.6863403  -0.78418374 -0.78865771 -0.75859024  0.85559254\n",
            "  -0.7936343  -0.79141752]\n",
            " [-0.00657025  0.00623397  0.24763384  0.2594021   1.00025458 -0.66581137\n",
            "  -0.27817962 -0.27261515 -0.29688365 -0.29858023 -0.29095711  0.28727323\n",
            "  -0.2961638  -0.29323103]\n",
            " [-0.02195656 -0.03365174 -0.00426546  0.02207729 -0.66581137  1.00025458\n",
            "   0.03519659  0.03787859  0.07497068  0.07746161  0.04734493 -0.05016567\n",
            "   0.06489226  0.06527952]\n",
            " [ 0.09182316  0.06806739 -0.69327904 -0.71366074 -0.27817962  0.03519659\n",
            "   1.00025458  0.99663082  0.9156677   0.90989004  0.98980432 -0.90453509\n",
            "   0.9059112   0.89902154]\n",
            " [ 0.09124849  0.06675938 -0.66619842 -0.6863403  -0.27261515  0.03787859\n",
            "   0.99663082  1.00025458  0.89978578  0.89804238  0.98135571 -0.8860481\n",
            "   0.89152389  0.88468357]\n",
            " [ 0.0906161   0.07330836 -0.76646982 -0.78418374 -0.29688365  0.07497068\n",
            "   0.9156677   0.89978578  1.00025458  0.99244327  0.9630022  -0.91668944\n",
            "   0.91434884  0.89781322]\n",
            " [ 0.09538375  0.07766039 -0.77169964 -0.78865771 -0.29858023  0.07746161\n",
            "   0.90989004  0.89804238  0.99244327  1.00025458  0.95682339 -0.91234956\n",
            "   0.91158665  0.89454192]\n",
            " [ 0.09382686  0.07201181 -0.73821112 -0.75859024 -0.29095711  0.04734493\n",
            "   0.98980432  0.98135571  0.9630022   0.95682339  1.00025458 -0.92963549\n",
            "   0.92909879  0.91904062]\n",
            " [-0.09974229 -0.0811853   0.84848979  0.85559254  0.28727323 -0.05016567\n",
            "  -0.90453509 -0.8860481  -0.91668944 -0.91234956 -0.92963549  1.00025458\n",
            "  -0.94086368 -0.94480617]\n",
            " [-0.1279056  -0.1498676  -0.78405759 -0.7936343  -0.2961638   0.06489226\n",
            "   0.9059112   0.89152389  0.91434884  0.91158665  0.92909879 -0.94086368\n",
            "   1.00025458  0.99448412]\n",
            " [-0.12235207 -0.14517775 -0.78201448 -0.79141752 -0.29323103  0.06527952\n",
            "   0.89902154  0.88468357  0.89781322  0.89454192  0.91904062 -0.94480617\n",
            "   0.99448412  1.00025458]]\n"
          ]
        }
      ],
      "source": [
        "#Calculating the Covariance Matrix\n",
        "cov_matrix = np.cov(standardized_data, rowvar=False)\n",
        "\n",
        "print(\"Covariance Matrix:\")\n",
        "print(cov_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Eigendecomposition on the Covariance Matrix\n"
      ],
      "metadata": {
        "id": "uXNcG4AFcT08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "print(\"Eigenvalues:\\n\", eigenvalues)\n",
        "print(\"Eigenvectors:\\n\", eigenvectors)"
      ],
      "metadata": {
        "id": "dmGlQ47tRO5w",
        "outputId": "b4988e2e-3483-4743-ae22-57257cce9af8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eigenvalues:\n",
            " [8.95720924e+00 2.06777956e+00 1.59364217e+00 6.70587666e-01\n",
            " 3.01684118e-01 1.61017548e-01 1.25550471e-01 6.40205207e-02\n",
            " 3.23528963e-02 1.36962433e-02 9.24627112e-03 4.24637785e-03\n",
            " 2.14358185e-03 3.87485886e-04]\n",
            "Eigenvectors:\n",
            " [[ 1.89952516e-02 -6.87675276e-01  7.65365183e-02  1.83729680e-02\n",
            "  -1.42115022e-02  7.02789695e-02  1.14861126e-01  4.01097554e-03\n",
            "  -1.15143583e-01 -6.95538857e-01 -6.83326435e-02  2.10633586e-02\n",
            "  -7.10578367e-03  6.07618642e-04]\n",
            " [ 1.24694065e-02 -6.89503292e-01  6.88439505e-02 -9.43831862e-03\n",
            "  -1.35600088e-02  3.37955333e-02  8.53170135e-02 -7.51768613e-03\n",
            "  -5.48830131e-02  7.10425085e-01  5.58369479e-02 -9.48384475e-03\n",
            "  -3.26885432e-04 -3.98397374e-03]\n",
            " [-2.81632694e-01  1.98550575e-02  5.61398643e-02  6.00933196e-01\n",
            "   9.71151716e-02 -7.09369930e-02  2.53817629e-01 -6.79764243e-01\n",
            "   1.19688616e-01  2.20121563e-03  1.99493288e-02  1.60174402e-02\n",
            "   8.04028190e-03 -8.90395340e-04]\n",
            " [-2.86142593e-01  3.10582044e-02  6.39803619e-02  5.69008152e-01\n",
            "  -1.12579685e-02  5.20085469e-04  2.41804525e-01  7.26076226e-01\n",
            "   5.26897639e-02  1.88707728e-02  2.64926668e-03  2.74345123e-03\n",
            "   1.81240405e-02  5.96415755e-03]\n",
            " [-1.13744496e-01 -6.35462585e-02 -6.68453743e-01  1.11921299e-01\n",
            "  -7.14324777e-01  5.74549175e-02 -8.32812952e-02 -5.31588314e-02\n",
            "  -1.69233476e-02 -1.20771098e-03 -5.08741222e-03  2.49515657e-03\n",
            "   4.77699628e-03  1.38002408e-03]\n",
            " [ 2.70758334e-02  8.29599394e-02  7.27541248e-01  4.21668361e-02\n",
            "  -6.60327363e-01  8.80109065e-02 -1.15893864e-01 -6.09407894e-02\n",
            "  -1.50666539e-02  6.07275003e-03 -7.58040546e-03  3.43747016e-03\n",
            "   5.48765707e-03  1.91014202e-03]\n",
            " [ 3.18051748e-01 -2.85135436e-02 -3.09457159e-02  3.07741602e-01\n",
            "   9.17288395e-02  2.33582113e-01 -3.48523745e-01 -1.55857376e-03\n",
            "   4.42386886e-03  1.69645875e-02 -1.66951941e-01  2.83920592e-02\n",
            "   4.69767835e-01 -6.06838753e-01]\n",
            " [ 3.13255816e-01 -2.82923311e-02 -2.88424193e-02  3.50190560e-01\n",
            "   9.64490942e-02  2.61685754e-01 -4.01063566e-01  5.82474590e-03\n",
            "  -5.01782609e-02 -1.87450161e-02  3.40247326e-01 -3.70704540e-02\n",
            "  -6.39659984e-01  9.87960517e-02]\n",
            " [ 3.22311434e-01 -2.87446531e-02 -7.41844009e-03  1.06756217e-01\n",
            "  -8.36095900e-02 -5.79051855e-01  6.98415760e-02  2.77125052e-02\n",
            "   9.29454003e-02  3.47168354e-02 -5.54437663e-01 -8.10568303e-02\n",
            "  -4.06961918e-01 -2.15560379e-01]\n",
            " [ 3.21798899e-01 -3.18825751e-02 -5.04723357e-03  9.25701948e-02\n",
            "  -8.81244175e-02 -5.98740052e-01  5.43149286e-02  3.27585244e-02\n",
            "  -3.73565517e-02 -6.93222596e-02  6.60371426e-01 -1.66112419e-02\n",
            "   2.69949131e-01 -3.33988596e-02]\n",
            " [ 3.26431467e-01 -2.95439904e-02 -2.46646202e-02  2.32059576e-01\n",
            "   3.57371029e-02 -3.52013759e-02 -1.98176493e-01  3.36046564e-04\n",
            "   2.43702958e-02  2.76273040e-02 -3.06928130e-01 -1.61455289e-03\n",
            "   3.55719959e-01  7.57771730e-01]\n",
            " [-3.24688616e-01  3.09636927e-02  2.93518765e-02  8.18837257e-02\n",
            "   7.40173982e-02 -2.55009838e-01 -3.27527105e-01 -2.56968080e-02\n",
            "  -8.31166274e-01  2.88398819e-02 -9.59897926e-02 -8.09763007e-02\n",
            "   1.25678109e-02 -3.29955487e-03]\n",
            " [ 3.21900973e-01  1.31945566e-01 -3.65682913e-02  4.07422491e-02\n",
            "  -4.52335010e-02  1.50377117e-01  4.11647519e-01 -1.91175338e-02\n",
            "  -3.95194409e-01  5.05543624e-02 -3.41858876e-02  7.19798863e-01\n",
            "  -5.88483770e-02 -4.22856834e-03]\n",
            " [ 3.19831952e-01  1.29271611e-01 -3.65195901e-02  2.92741212e-02\n",
            "  -4.98906660e-02  2.68563294e-01  4.79705745e-01 -3.49036362e-02\n",
            "  -3.25473709e-01  1.79402067e-02 -2.05372106e-03 -6.82266534e-01\n",
            "   3.17540349e-02 -9.96924525e-03]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Sort the Principal Components\n",
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list"
      ],
      "metadata": {
        "id": "4pWho88fcbJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "order_of_importance = np.argsort(eigenvalues)[::-1]\n",
        "print ( 'the order of importance is :\\n {}'.format(order_of_importance))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_znKtzdrTmMg",
        "outputId": "2d143558-ff37-4888-e20d-795b7a3d8d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the order of importance is :\n",
            " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question:\n",
        "\n",
        "1. Why do we order eigen values and eigen vectors?\n",
        "\n",
        "      answer=\n",
        "Eigenvalues and eigenvectors are ordered to simplify analysis and interpretation, especially in applications like PCA or solving differential equations. This ranking helps identify the most significant components. In PCA, ordering by eigenvalue magnitude aids in reducing dimensionality by keeping components with high variance (larger eigenvalues) and discarding those with less (smaller eigenvalues).\n",
        "\n",
        "2. Is it true we would consider the lowest eigen value compared to the highest? Defend your answer\n",
        "\n",
        "     answer=  We usually focus on the highest eigenvalues since they represent the most significant components, like in PCA, where larger eigenvalues capture key variance. However, the lowest eigenvalue can still be relevant in areas like stability analysis or optimization, where it may indicate weak directions or issues with matrix conditioning.\n"
      ],
      "metadata": {
        "id": "o1nILNGxpTJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You want to see what percentage of information each eigen value holds. You would have print out the percentage of each eigen value using the formula\n",
        "\n",
        "\n",
        "\n",
        "> (sorted eigen values / sum of all sorted eigen values) * 100\n",
        "\n"
      ],
      "metadata": {
        "id": "BWqFGNeNvgEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use sorted_eigenvalues to ensure the explained variances correspond to the eigenvectors\n",
        "order_of_importance = np.argsort(eigenvalues)[::-1]\n",
        "sorted_eigenvalues = eigenvalues[order_of_importance]\n",
        "\n",
        "sum_of_eigenvalues = np.sum(sorted_eigenvalues)\n",
        "\n",
        "print(sum_of_eigenvalues)\n",
        "\n",
        "percentages = (sorted_eigenvalues / sum_of_eigenvalues)\n",
        "\n",
        "print(percentages)\n",
        "\n",
        "for indexe, value in enumerate(sorted_eigenvalues):\n",
        "  print(f'Eigenvalue {indexe + 1}: {value: .8f}, with the percentage of: ({percentages[indexe]:.2f}%)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRMHrffrVOXR",
        "outputId": "f957421a-9c84-45a7-b142-e8787a2ce506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14.003564154786153\n",
            "[6.39637819e-01 1.47660948e-01 1.13802612e-01 4.78869279e-02\n",
            " 2.15433810e-02 1.14983262e-02 8.96560831e-03 4.57173046e-03\n",
            " 2.31033299e-03 9.78054100e-04 6.60279841e-04 3.03235505e-04\n",
            " 1.53074019e-04 2.76705189e-05]\n",
            "Eigenvalue 1:  8.95720924, with the percentage of: (0.64%)\n",
            "Eigenvalue 2:  2.06777956, with the percentage of: (0.15%)\n",
            "Eigenvalue 3:  1.59364217, with the percentage of: (0.11%)\n",
            "Eigenvalue 4:  0.67058767, with the percentage of: (0.05%)\n",
            "Eigenvalue 5:  0.30168412, with the percentage of: (0.02%)\n",
            "Eigenvalue 6:  0.16101755, with the percentage of: (0.01%)\n",
            "Eigenvalue 7:  0.12555047, with the percentage of: (0.01%)\n",
            "Eigenvalue 8:  0.06402052, with the percentage of: (0.00%)\n",
            "Eigenvalue 9:  0.03235290, with the percentage of: (0.00%)\n",
            "Eigenvalue 10:  0.01369624, with the percentage of: (0.00%)\n",
            "Eigenvalue 11:  0.00924627, with the percentage of: (0.00%)\n",
            "Eigenvalue 12:  0.00424638, with the percentage of: (0.00%)\n",
            "Eigenvalue 13:  0.00214358, with the percentage of: (0.00%)\n",
            "Eigenvalue 14:  0.00038749, with the percentage of: (0.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# use sorted_eigenvalues to ensure the explained variances correspond to the eigenvectors\n",
        "explained_variance = (sorted_eigenvalues / sum_of_eigenvalues) * 100\n",
        "explained_variance =[\"{:.2f}%\".format(value) for value in explained_variance]\n",
        "print( explained_variance)"
      ],
      "metadata": {
        "id": "pXcCtaDgToTQ",
        "outputId": "57136749-fcc7-4b39-f2e9-025729fbbeac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['63.96%', '14.77%', '11.38%', '4.79%', '2.15%', '1.15%', '0.90%', '0.46%', '0.23%', '0.10%', '0.07%', '0.03%', '0.02%', '0.00%']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize the number of Principle components then perfrom matrix multiplication with the variable K example k = 3 for 3 priciple components\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> The reulting matrix (with reduced data) = standardized data * vector with columns k\n",
        "\n",
        "See expected output for k = 2\n",
        "\n"
      ],
      "metadata": {
        "id": "qB7H4InbfKx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 3\n",
        "top_k_eigenvectors = eigenvectors[:, :k]\n",
        "reduced_data = np.matmul(standardized_data, eigenvectors[:, :k])"
      ],
      "metadata": {
        "id": "C-Rnyq6QVTiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxxBcgQMXe1h",
        "outputId": "700a851f-8227-4082-9c66-d32ce8de40fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-3.19146207  1.98769416 -1.85142805]\n",
            " [ 0.38752701  1.99194578 -2.47970321]\n",
            " [-2.09148498  2.03743394 -2.20581408]\n",
            " ...\n",
            " [ 6.84833724 -2.12577829  0.79243213]\n",
            " [-0.99151331 -2.51764543 -1.88248871]\n",
            " [-1.99449758 -2.60766847 -1.80723835]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEqS6cuaMSY",
        "outputId": "17dc27ac-cbbb-42b8-c2f2-1b198879048a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3929, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *What are 2 positive effects and 2 negative effects of PCA\n",
        "\n",
        "Give 2 Benefits and 2 limitations\n",
        "[insert answer here]"
      ],
      "metadata": {
        "id": "UxQ8lTunauMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limitations (Negative Effects):\n",
        "\n",
        "\n",
        "Loss of Information: PCA aims to retain as much variance as possible when reducing dimensionality. However, in the process, some information may be lost. If the variance of the data is concentrated in dimensions that are removed, this can result in a significant loss of information.\n",
        "\n",
        "Loss of Interpretability: After applying PCA, the new feature space is a linear combination of the original features. This can lead to a loss of interpretability, making it challenging to relate the reduced dimensions back to the original features. Understanding the meaning of the principal components can be complex.\n",
        "\n",
        "Benefits (Positive Effects):\n",
        "\n",
        "\n",
        "Noise Reduction/Feature Extraction: PCA can help in reducing the impact of noise or irrelevant features in the data. By focusing on the principal components that capture the most variance, you can suppress the influence of less important dimensions, which can improve the performance of machine learning algorithms, and help to identify patterns and trends in the data.\n",
        "\n",
        "Dimensionality Reduction: PCA can reduce the dimensionality of the data without losing too much information. By selecting a subset of the most important principal components, you can simplify complex datasets, making them easier to visualize, analyze, and model."
      ],
      "metadata": {
        "id": "TaXP4mngRaed"
      }
    }
  ]
}